E,v
             #'B4':             #   {'center_wcs': [(3, 44, 18), (32, 05, 20)],             #    'map': None,             #    'threshold': None,             #    'box_wcs': None,             #    },,v
dust gas correlation variation molecular cloud,v
0,v
\label{fig:pdfs},v
,V
 ,v
_,v
    A non-parametric approach to determining the error on the HI velocity range    given that the N(HI) is non-linearly dependent on the HI velocity range. ,v
http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_discrete.html,v
n,v
r,v
    #vel_error = cores[core]['hi_velocity_range_error'],v
    print(nhi_error),v
    nhi_error = np.median(results_dict['nhi errors']),v
,},v
        mask = myg.get_polygon_mask(av_data_planck_orig,                cores[core]['box_vertices_rotated']),v
e,v
            print hi_vel_range_noise,v
+,v
            hi_vel_range_list.append(hi_vel_range),v
,v
),v
np.asarray(hi_vel_range),v
#,v
    vel_error = cores[core]['hi_velocity_range_error'],v
    for figure_type in figure_types:        if hi_co_width or hi_av_correlation:            plot_co_spectrum_grid(co_vel_axis,                    co_image_list,                    vel_range_list=hi_vel_range_list,                    vel_range_hiav_list=hi_vel_range_corr_list,                    #limits = [0, 80, 10**-3, 10**2],                    savedir = figure_dir + 'panel_cores/',                    scale = ('linear', 'linear'),                    filename = 'perseus_core_co_spectra_grid.%s' % figure_type,                    title = r'Average $^{12}$CO spectra of perseus Cores',                    core_names=core_name_list,                    show = False),v
    co_width_scale = 5.0 # for determining N(HI) vel range    # 0.758 is fraction of area of Gaussian between FWHM limits    co_flux_fraction = 0.758 # fraction of flux of average CO spectrum    hi_vel_range_scale = 1.0 # scale hi velocity range for Av/N(HI) correlation,v
    reproduce_lee12 = True,v
    cores[core]['hi_velocity_range'],v
2,v
Elijah - when doing bootstrapping please check how many iterations should be done so that you are not oversampling the parameter distribution. I also need some more explanation here re exactly what's being done (e.g. difference btw bootstrapping and Monte Carlo - do you use both methods here or just one?).,v
Orientation,v
        # Annotations        anno_xpos = 0.95,v
        if phi_cnm_list is not None:            phi_cnm = phi_cnm_list[i]        if phi_cnm_error_list is not None:            phi_cnm_error = phi_cnm_error_list[i]        if Z_list is not None:            Z = Z_list[i]        if Z_error_list is not None:            Z_error = Z_error_list[i]        if chisq_list is not None:            chisq = chisq_list[i]        if p_value_list is not None:            p_value = p_value_list[i],v
fit=True, ,v
,        phi_cnm_list=None, phi_cnm_error_list=None, Z_list=None,        Z_error_list=None, chisq_list=None, p_value_list=None,,v
$\Sigma_{HI}$ + $\Sigma_{H2}$ (M$_\odot$ / pc$^2$),v
h,v
'perseus_NGC1333_phi_cnm_hist.png',v
http://www.prepressure.com/postscript/troubleshooting/errors/image,v
B,v
3,v
        plt.show(),v
m,v
http://xkcd.com/,v
pl.hist(data, bins=np.logspace(0.1, 1.0, 50))pl.gca().set_xscale("log"),v
1,v
5,v
phi_cnm,v
g,v
b,v
        ax.plot(width_correlations_recreate, 100, alpha=0.3,                label='Widths Reproduced', color='b', normed=True),v
             label='Reproduced', color='b', normed=True),v
$\phi_{\rm CNM}$,v
t,v
    #plt.hist(center_correlations)    #plt.plot(width_correlations)    #plt.plot(center_correlations)    #plt.show(),v
x,v
(,v
    plt.hist(results_dict['phi_cnm fits'])    plt.save(),v
        if verbose:            print('HI velocity integration range:')            print('%.0f to %.0f km/s' % (hi_vel_range[0], hi_vel_range[1])),v
# # # # ,v
          ,v
# Bootstrapping using means,v
    phi_cnm = phi_cnm,v
    hi_vel_range_sample = (np.median(hi_vel_range_list[:, 0]),                           np.median(hi_vel_range_list[:, 1])),v
hi_vel_range_sample, ,v
hi_vel_range, ,v
np.median(hi_vel_range_list[:, 0]),                                        np.median(hi_vel_range_list[:, 1])),v
np.median(hi_vel_range_list[:, 0]),                                        np.median(hi_vel_range_list[:, 1]),v
    import matplotlib.pyplot as plt,v
    import mystats,v
    import random,v
    #center_corrs = [],v
    #raise ValueError('done'),v
            #vel_center_random = center_pdf(random_center_sample)            #vel_width_random = width_pdf(random_width_sample),v
pdf(random_width_sample),v
    	print(center_rv.rvs()),v
    #    center = random.uniform(vel_centers[0], vel_centers[-1] )    #    center_correlations_recreate[i] = center_pdf(center)    #for i in range(len(center_correlations_recreate)):    #    center = random.uniform(center_correlations.min(),    #                            center_correlations.max())    #    print center    #    center_correlations_recreate[i] = center_pdf(center)    #for vel_center in vel_centers:    #    center_corrs.append(center_pdf(vel_center))    # plot both    #plt.plot(vel_centers, center_correlations, label='Observed', color='r'),v
.min().max(),v
b = numpy.zeros(10000)for i in range(len( b )):    u = random.uniform( x[0], x[-1] )    b[i] = inverse_density_function( u )# plot both        pyplot.hist(a, 100) pyplot.hist(b, 100)pyplot.show(),v
http://voxcharta.org/,v
            print(hi_vel_range_noise),v
hi_vel_range_list[:, 0],v
[:, 0],v
],v
np.toarray(hi_vel_range_list)),v
s,v
a,v
        print('median of data', np.median(hi_data_sub)),v
4,v
@,v
import numpyimport scipy.interpolateimport randomimport matplotlib.pyplot as pyplot# create some normally distributed values and make a histograma = numpy.random.normal(size=10000)counts, bins = numpy.histogram(a, bins=100, density=True)cum_counts = numpy.cumsum(counts)bin_widths = (bins[1:] - bins[:-1])# generate more values with same distributionx = cum_counts*bin_widthsy = bins[1:]inverse_density_function = scipy.interpolate.interp1d(x, y)b = numpy.zeros(10000)for i in range(len( b )):    u = random.uniform( x[0], x[-1] )    b[i] = inverse_density_function( u )# plot both        pyplot.hist(a, 100) pyplot.hist(b, 100)pyplot.show(),v
    print(help(center_pdf)),v
    print len(center_correlations), len(vel_centers),v
.,v
PYTHONPATH=$PYTHONPATH:,v
My thinking is that a s,v
I wrote an algorithm to rotate a box pivoted on the location of the core through many angles from 0 to 360 degrees. For each box I derived the azimuthally-averaged Av values, calculated the gradient of the Av between each azimuthal ring, added the gradient values together, and counted the box with the minimum sum of gradient values as the box with the steepest Av gradient. My thinking is that a steeper Av gradient will correspond to an isolated core vs. a box with multiple cores included. This method seems pretty sound to me and can easily be applied across all clouds in a reproducible manner.,v
http://stackoverflow.com/questions/13476807/probability-density-function-from-histogram-in-python-to-fit-another-histrogram,v
